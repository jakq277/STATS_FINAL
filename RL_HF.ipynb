{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h6wxlrA6p9g",
        "outputId": "ed6d8651-3b80-445d-ab1e-bc9beb6492c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: be-great in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.8.6)\n",
            "Requirement already satisfied: numpy>=1.23.1 in /usr/local/lib/python3.10/dist-packages (from be-great) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from be-great) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from be-great) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from be-great) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from be-great) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.20.1 in /usr/local/lib/python3.10/dist-packages (from be-great) (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.1->be-great) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.4->be-great) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.4->be-great) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.4->be-great) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.1->be-great) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.1->be-great) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.1->be-great) (3.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->be-great) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.2->be-great) (12.5.40)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.4->be-great) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.2->be-great) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.2->be-great) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install be-great datasets transformers trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWrM5yCYS21"
      },
      "source": [
        "## Step 0: load dataset\n",
        "\n",
        "First we load the table we want to synthesize."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teBDxiXd9Pqv",
        "outputId": "6895e37a-a401-4a1e-c625-bce703a6c26b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/gdrive/MyDrive/stats261/ctr_subset.csv\""
      ],
      "metadata": {
        "id": "N72uHAKe9lKm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GlgwmbSL-JQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YjUH_pxfYVAV"
      },
      "outputs": [],
      "source": [
        "# replace with actual csv loading\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_path)\n",
        "#data = data.sample(100)\n",
        "real_columns = data.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIIaL3X386HB",
        "outputId": "b0ef44e2-d3cb-4bee-ce45-4a30f9efbe0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(238272, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSlw-P7NYHIy"
      },
      "source": [
        "## Step 1: supervised-finetuning for table generation\n",
        "\n",
        "In this step, we finetune a distillgpt2 model to perform synthetic table generation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gpHJZuny_VsH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "w5zPcP2vYGog",
        "outputId": "ce060eac-b32e-4b70-9c78-d5a1cc7a1469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1545' max='74460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1545/74460 17:24 < 13:42:54, 1.48 it/s, Epoch 0.21/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.950800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.773800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.749500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from be_great import GReaT\n",
        "\n",
        "\n",
        "duration = 10\n",
        "\n",
        "model_great = GReaT(llm='distilgpt2', batch_size=32,  epochs=duration, fp16=True)\n",
        "model_great.fit(data)\n",
        "base_model = model_great.model\n",
        "base_model.save_pretrained(\"./trained_base_model\")\n",
        "synthetic_data = model_great.sample(n_samples=100,max_length=duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vrgAcTLYyU9"
      },
      "source": [
        "## Step 2: reward model training\n",
        "\n",
        "Our reward model is a powerful classifier trained on the real tabular data. We apply it on synthetic table rows, and the reward is maximize when the distance between synthetic and predicted class probabilities are minimized. The idea is that the synthetic data should preserved the feature-target relationship as found in the real data, by powerful classsifiers such as XGboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic4mtv_JY1Rd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1] # assume y is already encoded as 0 and 1\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "\n",
        "# Define the model and preprocessors\n",
        "classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation as sanity check\n",
        "y_pred = pipeline.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzfTRB1WZ6IH"
      },
      "source": [
        "## Step 3: reward finetuning\n",
        "\n",
        "Now we define a reward function that does the following\n",
        "\n",
        "\n",
        "1.   Given a generated text, reverse it back to a table row.\n",
        "2.   Use the pre-trained classifier to predict its target based generated features.\n",
        "3.   Given the distance between predicted and synthetic targets, calculate its reward.\n",
        "\n",
        "Then we finetune the model trained above in PPO setting.\n",
        "\n",
        "Cross check to see if PPO and RL work correctly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQdbycVto_DN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from be_great.great_utils import _convert_text_to_tabular_data, _convert_tokens_to_text\n",
        "\n",
        "\n",
        "def calcualte_reward(synth_data, classifier, preprocessor):\n",
        "    X_synth = synth_data.iloc[:, :-1]\n",
        "    y_synth = synth_data.iloc[:, -1]\n",
        "\n",
        "    # Apply preprocessing pipeline\n",
        "    #print(X_synth)\n",
        "    X_synth_transformed = preprocessor.transform(X_synth)\n",
        "\n",
        "    # Get predicted class probabilities\n",
        "    y_pred_proba = classifier.predict_proba(X_synth_transformed)[:, 1]\n",
        "\n",
        "    rewards = 1 - np.abs(y_pred_proba - y_synth)\n",
        "\n",
        "    # Format rewards for PPO trainer\n",
        "    # the PPO trainer expects a list of tensors\n",
        "    return [torch.tensor(r) for r in rewards]\n",
        "\n",
        "# Custom reward function\n",
        "def reward_function(output_text, clasifier, preprocessor, columns, num_cols):\n",
        "    # Replace with your actual reward computation logic\n",
        "    synth_data = _convert_text_to_tabular_data(output_text, columns)\n",
        "    # Remove rows where we have not generated anything\n",
        "    synth_data = synth_data[~(synth_data == \"placeholder\").any(axis=1)]\n",
        "\n",
        "    # Remove rows where all values are NaN\n",
        "    synth_data = synth_data.dropna(how=\"all\")\n",
        "\n",
        "    # Remove rows with flawed numerical values but keep NaNs\n",
        "    #print(len(num_cols), synth_data.shape)\n",
        "    for i_num_cols in num_cols:\n",
        "        coerced_series = pd.to_numeric(\n",
        "            synth_data[i_num_cols], errors=\"coerce\"\n",
        "        )\n",
        "        synth_data = synth_data[\n",
        "            coerced_series.notnull() | synth_data[i_num_cols].isna()\n",
        "        ]\n",
        "\n",
        "    # Convert numerical columns to float\n",
        "    synth_data[num_cols] = synth_data[num_cols].astype(float)\n",
        "    if len(synth_data) > 0:\n",
        "        return calcualte_reward(synth_data, clasifier, preprocessor)\n",
        "    else:\n",
        "        return [torch.tensor(0.0, dtype=torch.float32)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTpKJsBgtRSa"
      },
      "outputs": [],
      "source": [
        "def df_to_string_list(df):\n",
        "    string_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        row_string = \", \".join([f\"{col} is {val}\" for col, val in row.items()])\n",
        "        string_list.append(row_string)\n",
        "    return string_list\n",
        "\n",
        "def df_cells_to_string_list(df):\n",
        "    cell_string_list = []\n",
        "    for col in df.columns:\n",
        "        for val in df[col]:\n",
        "            cell_string_list.append(f\"{col} is {val}\")\n",
        "    return cell_string_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEeOHfCl6JbO"
      },
      "outputs": [],
      "source": [
        "df_text = df_to_string_list(synthetic_data)\n",
        "\n",
        "# Test reconstruction of table from text\n",
        "rewards = reward_function(df_text, classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2aBWGqiZ9Bl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "llm = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the trained base model into AutoModelForCausalLMWithValueHead\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"./trained_base_model\")\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Define dataset. Use different columns as condition to improve diversity\n",
        "dataset = CustomDataset(df_cells_to_string_list(data))\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define PPO Configuration\n",
        "ppo_config = PPOConfig(\n",
        "    model_name=llm,\n",
        "    learning_rate=5e-6,\n",
        "    batch_size=batch_size,\n",
        "    ppo_epochs=4,\n",
        "    mini_batch_size=16,\n",
        "    gradient_accumulation_steps=2\n",
        ")\n",
        "\n",
        "# Initialize the PPO Trainer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=model,\n",
        "    ref_model=None,  # Reference model\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSrnaev_g1XM"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# PPO Training loop\n",
        "\n",
        "#https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
        "\n",
        "for epoch in range(ppo_config.ppo_epochs):\n",
        "    epoch_rewards = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        try:\n",
        "            # Tokenize inputs and move to device\n",
        "            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "\n",
        "            # Generate outputs from the model\n",
        "            # Set max_length to the desired length of the generated text\n",
        "            max_length = 300  # Adjust as needed for your application\n",
        "            generated_ids = model.generate(input_ids, max_length=max_length, do_sample=True,temperature=0.70, pad_token_id=50256)\n",
        "\n",
        "            generated_texts = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(generated_ids.size(0))]\n",
        "\n",
        "            rewards = [reward_function([generated_text], classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]]) for generated_text in generated_texts]\n",
        "\n",
        "            rewards = [item for sublist in rewards for item in sublist]\n",
        "\n",
        "            epoch_rewards.extend(rewards)\n",
        "\n",
        "            queries = [input_ids[i] for i in range(input_ids.size(0))]\n",
        "            responses = [generated_ids[i] for i in range(generated_ids.size(0))]\n",
        "            rewards = [reward.clone().detach() for reward in rewards]\n",
        "\n",
        "            ppo_trainer.step(queries, responses, rewards)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    avg_reward = torch.mean(torch.stack(epoch_rewards)).item()\n",
        "    print(f\"Epoch: {epoch+1}/{ppo_config.ppo_epochs}, Average Reward: {avg_reward}\")\n",
        "\n",
        "print(\"PPO training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu25Eastzwwp"
      },
      "source": [
        "## Step 4 (TODO)\n",
        "\n",
        "Finally we load the trained parameters back to GReaT model, generate synthetic data, train another XGBoost on new synthtic data and observe changes its utility"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_great.parameters = model.parameters\n",
        "new_synthetic_data = model_great.sample(n_samples=100,max_length=duration)\n",
        "\n",
        "new_df_text = df_to_string_list(new_synthetic_data)\n",
        "\n",
        "new_rewards = reward_function(df_text, classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]])\n",
        "\n",
        "\n",
        "new_classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', new_classifier)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluation as sanity check\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "za5h7cGl8LaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}