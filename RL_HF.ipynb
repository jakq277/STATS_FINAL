{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h6wxlrA6p9g",
        "outputId": "ed6d8651-3b80-445d-ab1e-bc9beb6492c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: be-great in /opt/conda/envs/llm/lib/python3.10/site-packages (0.0.7)\n",
            "Requirement already satisfied: datasets in /opt/conda/envs/llm/lib/python3.10/site-packages (2.14.6)\n",
            "Requirement already satisfied: transformers in /opt/conda/envs/llm/lib/python3.10/site-packages (4.34.1)\n",
            "Requirement already satisfied: trl in /opt/conda/envs/llm/lib/python3.10/site-packages (0.8.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sdmetric (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sdmetric\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install be-great datasets transformers trl sdmetric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWrM5yCYS21"
      },
      "source": [
        "## Step 0: load dataset\n",
        "\n",
        "First we load the table we want to synthesize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teBDxiXd9Pqv",
        "outputId": "6895e37a-a401-4a1e-c625-bce703a6c26b"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N72uHAKe9lKm"
      },
      "outputs": [],
      "source": [
        "#data_path = \"/content/gdrive/MyDrive/stats261/ctr_subset.csv\"\n",
        "data_path = \"ctr_subset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YjUH_pxfYVAV"
      },
      "outputs": [],
      "source": [
        "# replace with actual csv loading\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv(data_path,index_col=0)\n",
        "data = data.sample(2142,random_state=42)\n",
        "\n",
        "\n",
        "# Adjust order of columns to make target label the last\n",
        "labels = data.pop('label')\n",
        "data['label'] = labels\n",
        "\n",
        "real_columns = data.columns\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIIaL3X386HB",
        "outputId": "b0ef44e2-d3cb-4bee-ce45-4a30f9efbe0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2142, 24) Index(['age', 'residence', 'city', 'emui_dev', 'device_name', 'device_size',\n",
            "       'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id', 'slot_id',\n",
            "       'spread_app_id', 'hispace_app_tags', 'app_second_class', 'pt_d',\n",
            "       'u_refreshTimes_x', 'u_feedLifeCycle_y', 'u_refreshTimes_y', 'i_cat',\n",
            "       'i_upTimes', 'e_m', 'e_pl', 'user_id', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(data.shape, data.columns)\n",
        "data.to_csv(\"ctrBalanced2142.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSlw-P7NYHIy"
      },
      "source": [
        "## Step 1: supervised-finetuning for table generation\n",
        "\n",
        "In this step, we finetune a distillgpt2 model to perform synthetic table generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "w5zPcP2vYGog",
        "outputId": "ce060eac-b32e-4b70-9c78-d5a1cc7a1469"
      },
      "outputs": [],
      "source": [
        "from be_great import GReaT\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "duration = 500\n",
        "max_seq_len = 500\n",
        "\n",
        "trained_checkpoint = None\n",
        "#trained_checkpoint = \"./great_checkpoint\"\n",
        "\n",
        "model_great = GReaT(llm='distilgpt2', batch_size=32,  epochs=duration, fp16=True,save_steps=30000)\n",
        "if trained_checkpoint is not None:\n",
        "    model_great.load_from_dir(trained_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27000' max='27000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27000/27000 2:02:36, Epoch 500/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.936700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.751500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.718100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.697400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.680800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.666400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.633300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.617500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.595400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.586900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.566900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.556200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.551300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.547000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.542600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.538200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.534400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.523300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.519800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.516600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.513200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.510300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.507300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.504200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.501400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.498900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.496800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.492000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.489600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.488000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.485900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.484200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.482000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.480900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.479700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.477900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.476800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.475800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.474800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.472900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.472100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.471100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.470800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.471100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<be_great.great_trainer.GReaTTrainer at 0x7f4d0f9eace0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_great.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = model_great.model\n",
        "base_model.save_pretrained(\"./trained_base_model_2142\")\n",
        "model_great.save(\"./great_checkpoint_2142\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1729it [00:42, 41.15it/s]                          \n"
          ]
        }
      ],
      "source": [
        "synthetic_data = model_great.sample(n_samples=len(train),max_length=max_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "synthetic_data.to_csv(\"ctr_GReaT_default_2142.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vrgAcTLYyU9"
      },
      "source": [
        "## Step 2: reward model training\n",
        "\n",
        "Our reward model is a powerful classifier trained on the real tabular data. We apply it on synthetic table rows, and the reward is maximize when the distance between synthetic and predicted class probabilities are minimized. The idea is that the synthetic data should preserved the feature-target relationship as found in the real data, by powerful classsifiers such as XGboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ic4mtv_JY1Rd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.668997668997669\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.65      0.65       200\n",
            "           1       0.69      0.69      0.69       229\n",
            "\n",
            "    accuracy                           0.67       429\n",
            "   macro avg       0.67      0.67      0.67       429\n",
            "weighted avg       0.67      0.67      0.67       429\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "\n",
        "X_train, X_test = train.iloc[:, :-1], test.iloc[:, :-1]\n",
        "y_train, y_test = train.iloc[:, -1],  test.iloc[:, -1]  # assume y is already encoded as 0 and 1\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "\n",
        "# Define the model and preprocessors\n",
        "classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier)])\n",
        "\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation as sanity check\n",
        "y_pred = pipeline.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic Accuracy: 0.6503496503496503\n",
            "Synthetic Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63       200\n",
            "           1       0.68      0.66      0.67       229\n",
            "\n",
            "    accuracy                           0.65       429\n",
            "   macro avg       0.65      0.65      0.65       429\n",
            "weighted avg       0.65      0.65      0.65       429\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now try fitting a model from synthetic data and evaluate on test set\n",
        "x_synth, y_synth = synthetic_data.iloc[:, :-1],synthetic_data.iloc[:, -1]\n",
        "\n",
        "classifier_synth = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline_synth = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier_synth)])\n",
        "pipeline_synth.fit(x_synth, y_synth)\n",
        "y_pred = pipeline_synth.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Synthetic Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Synthetic Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzfTRB1WZ6IH"
      },
      "source": [
        "## Step 3: reward finetuning\n",
        "\n",
        "Now we define a reward function that does the following\n",
        "\n",
        "\n",
        "1.   Given a generated text, reverse it back to a table row.\n",
        "2.   Use the pre-trained classifier to predict its target based generated features.\n",
        "3.   Given the distance between predicted and synthetic targets, calculate its reward.\n",
        "\n",
        "Then we finetune the model trained above in PPO setting.\n",
        "\n",
        "Cross check to see if PPO and RL work correctly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eQdbycVto_DN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from be_great.great_utils import _convert_text_to_tabular_data, _convert_tokens_to_text\n",
        "\n",
        "\n",
        "def calcualte_reward(synth_data, classifier, preprocessor):\n",
        "    X_synth = synth_data.iloc[:, :-1]\n",
        "    y_synth = synth_data.iloc[:, -1]\n",
        "\n",
        "    # Apply preprocessing pipeline\n",
        "    #print(X_synth)\n",
        "    X_synth_transformed = preprocessor.transform(X_synth)\n",
        "\n",
        "    # Get predicted class probabilities\n",
        "    y_pred_proba = classifier.predict_proba(X_synth_transformed)[:, 1]\n",
        "\n",
        "    rewards = 1 - np.abs(y_pred_proba - y_synth)\n",
        "\n",
        "    # Format rewards for PPO trainer\n",
        "    # the PPO trainer expects a list of tensors\n",
        "    return [torch.tensor(r) for r in rewards]\n",
        "\n",
        "# Custom reward function\n",
        "def reward_function(output_text, clasifier, preprocessor, columns, num_cols):\n",
        "    # Replace with your actual reward computation logic\n",
        "    synth_data = _convert_text_to_tabular_data(output_text, columns)\n",
        "    # Remove rows where we have not generated anything\n",
        "    synth_data = synth_data[~(synth_data == \"placeholder\").any(axis=1)]\n",
        "\n",
        "    # Remove rows where all values are NaN\n",
        "    synth_data = synth_data.dropna(how=\"all\")\n",
        "\n",
        "    # Remove rows with flawed numerical values but keep NaNs\n",
        "    #print(len(num_cols), synth_data.shape)\n",
        "    for i_num_cols in num_cols:\n",
        "        coerced_series = pd.to_numeric(\n",
        "            synth_data[i_num_cols], errors=\"coerce\"\n",
        "        )\n",
        "        synth_data = synth_data[\n",
        "            coerced_series.notnull() | synth_data[i_num_cols].isna()\n",
        "        ]\n",
        "\n",
        "    # Convert numerical columns to float\n",
        "    synth_data[num_cols] = synth_data[num_cols].astype(float)\n",
        "    if len(synth_data) > 0:\n",
        "        return calcualte_reward(synth_data, clasifier, preprocessor)\n",
        "    else:\n",
        "        return [torch.tensor(0.0, dtype=torch.float32)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oTpKJsBgtRSa"
      },
      "outputs": [],
      "source": [
        "def df_to_string_list(df):\n",
        "    string_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        row_string = \", \".join([f\"{col} is {val}\" for col, val in row.items()])\n",
        "        string_list.append(row_string)\n",
        "    return string_list\n",
        "\n",
        "def df_cells_to_string_list(df):\n",
        "    cell_string_list = []\n",
        "    for col in df.columns:\n",
        "        for val in df[col]:\n",
        "            cell_string_list.append(f\"{col} is {val}\")\n",
        "    return cell_string_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XEeOHfCl6JbO"
      },
      "outputs": [],
      "source": [
        "df_text = df_to_string_list(synthetic_data)\n",
        "\n",
        "# Test reconstruction of table from text\n",
        "rewards = reward_function(df_text, classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m2aBWGqiZ9Bl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "llm = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the trained base model into AutoModelForCausalLMWithValueHead\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"./trained_base_model_2142\")\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Define dataset. Use different columns as condition to improve diversity\n",
        "dataset = CustomDataset(df_cells_to_string_list(train))\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define PPO Configuration\n",
        "ppo_config = PPOConfig(\n",
        "    model_name=llm,\n",
        "    learning_rate=1e-5,  # Slightly increased learning rate\n",
        "    batch_size=batch_size,\n",
        "    ppo_epochs=4,\n",
        "    mini_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "# Initialize the PPO Trainer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=model,\n",
        "    ref_model=None,  # Reference model\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xSrnaev_g1XM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/4, Average Reward: 0.013847221620380878\n",
            "Epoch: 2/4, Average Reward: 0.0\n",
            "Epoch: 3/4, Average Reward: 0.0\n",
            "Epoch: 4/4, Average Reward: 0.0\n",
            "PPO training completed!\n"
          ]
        }
      ],
      "source": [
        "# PPO Training loop\n",
        "\n",
        "#https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "for epoch in range(ppo_config.ppo_epochs):\n",
        "    epoch_rewards = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        try:\n",
        "            # Tokenize inputs and move to device\n",
        "            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "\n",
        "            # Generate outputs from the model\n",
        "            # Set max_length to the desired length of the generated text\n",
        "            max_length = 300  # Adjust as needed for your application\n",
        "            generated_ids = model.generate(input_ids, max_length=max_length, do_sample=True,temperature=0.70, pad_token_id=50256)\n",
        "\n",
        "            generated_texts = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(generated_ids.size(0))]\n",
        "\n",
        "            rewards = [reward_function([generated_text], classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]]) for generated_text in generated_texts]\n",
        "\n",
        "            rewards = [item for sublist in rewards for item in sublist]\n",
        "\n",
        "            epoch_rewards.extend(rewards)\n",
        "\n",
        "            queries = [input_ids[i] for i in range(input_ids.size(0))]\n",
        "            responses = [generated_ids[i] for i in range(generated_ids.size(0))]\n",
        "            rewards = [reward.clone().detach() for reward in rewards]\n",
        "\n",
        "            ppo_trainer.step(queries, responses, rewards)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    avg_reward = torch.mean(torch.stack(epoch_rewards)).item()\n",
        "    print(f\"Epoch: {epoch+1}/{ppo_config.ppo_epochs}, Average Reward: {avg_reward}\")\n",
        "\n",
        "print(\"PPO training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_directory = \"RL_trained\"\n",
        "model.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu25Eastzwwp"
      },
      "source": [
        "## Step 4 (TODO)\n",
        "\n",
        "Finally we load the trained parameters back to GReaT model, generate synthetic data, train another XGBoost on new synthtic data and observe changes its utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at RL_trained were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Run this cell if we are returning after RL training.\n",
        "save_directory = \"RL_trained\"\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(save_directory)\n",
        "\n",
        "llm = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "za5h7cGl8LaZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1747it [00:41, 42.22it/s]                          \n"
          ]
        }
      ],
      "source": [
        "model_great.parameters = model.parameters\n",
        "new_synthetic_data = model_great.sample(n_samples=len(train),max_length=duration)\n",
        "\n",
        "new_df_text = df_to_string_list(new_synthetic_data)\n",
        "\n",
        "new_rewards = reward_function(df_text, classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_synthetic_data.to_csv(\"ctr_GReaTRL_default_2142.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6363636363636364\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.59      0.60       200\n",
            "           1       0.66      0.67      0.66       229\n",
            "\n",
            "    accuracy                           0.64       429\n",
            "   macro avg       0.63      0.63      0.63       429\n",
            "weighted avg       0.64      0.64      0.64       429\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_synth_new, y_synth_new = new_synthetic_data.iloc[:,:-1], new_synthetic_data.iloc[:,-1]\n",
        "\n",
        "classifier_synth = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline_synth = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier_synth)])\n",
        "\n",
        "pipeline_synth.fit(x_synth_new, y_synth_new)\n",
        "y_pred = pipeline_synth.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "# Evaluation as sanity check\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>residence</th>\n",
              "      <th>city</th>\n",
              "      <th>emui_dev</th>\n",
              "      <th>device_name</th>\n",
              "      <th>device_size</th>\n",
              "      <th>task_id</th>\n",
              "      <th>adv_id</th>\n",
              "      <th>creat_type_cd</th>\n",
              "      <th>adv_prim_id</th>\n",
              "      <th>...</th>\n",
              "      <th>pt_d</th>\n",
              "      <th>u_refreshTimes_x</th>\n",
              "      <th>u_feedLifeCycle_y</th>\n",
              "      <th>u_refreshTimes_y</th>\n",
              "      <th>i_cat</th>\n",
              "      <th>i_upTimes</th>\n",
              "      <th>e_m</th>\n",
              "      <th>e_pl</th>\n",
              "      <th>user_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>31941.0</td>\n",
              "      <td>21695.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1277.0</td>\n",
              "      <td>1156.0</td>\n",
              "      <td>269004.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>35422.0</td>\n",
              "      <td>12785.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1319.0</td>\n",
              "      <td>1197.0</td>\n",
              "      <td>104736.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>2032.0</td>\n",
              "      <td>34041.0</td>\n",
              "      <td>13608.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>591.0</td>\n",
              "      <td>2952.0</td>\n",
              "      <td>203902.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>2438.0</td>\n",
              "      <td>36511.0</td>\n",
              "      <td>22799.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1189.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>1701.0</td>\n",
              "      <td>111729.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>429.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>334.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>31941.0</td>\n",
              "      <td>21695.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1481.0</td>\n",
              "      <td>687.0</td>\n",
              "      <td>102986.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1708</th>\n",
              "      <td>3.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>292.0</td>\n",
              "      <td>1891.0</td>\n",
              "      <td>31706.0</td>\n",
              "      <td>12646.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1922.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>2.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>848.0</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>105735.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1709</th>\n",
              "      <td>6.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>1656.0</td>\n",
              "      <td>31941.0</td>\n",
              "      <td>21695.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1277.0</td>\n",
              "      <td>1255.0</td>\n",
              "      <td>134502.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1710</th>\n",
              "      <td>8.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>1787.0</td>\n",
              "      <td>16068.0</td>\n",
              "      <td>13876.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1234.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1075.0</td>\n",
              "      <td>2073.0</td>\n",
              "      <td>219349.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1711</th>\n",
              "      <td>8.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>278.0</td>\n",
              "      <td>2482.0</td>\n",
              "      <td>33257.0</td>\n",
              "      <td>15452.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1779.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>506.0</td>\n",
              "      <td>1422.0</td>\n",
              "      <td>275034.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1712</th>\n",
              "      <td>3.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>26315.0</td>\n",
              "      <td>18051.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1482.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>2620.0</td>\n",
              "      <td>137423.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1713 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  residence   city  emui_dev  device_name  device_size  task_id  \\\n",
              "0     7.0       20.0  170.0      35.0        319.0       2117.0  31941.0   \n",
              "1     5.0       29.0  207.0      35.0        324.0       2401.0  35422.0   \n",
              "2     3.0       16.0  110.0      21.0        151.0       2032.0  34041.0   \n",
              "3     8.0       20.0  170.0      30.0        228.0       2438.0  36511.0   \n",
              "4     8.0       39.0  429.0      20.0        334.0       2117.0  31941.0   \n",
              "...   ...        ...    ...       ...          ...          ...      ...   \n",
              "1708  3.0       33.0  319.0      13.0        292.0       1891.0  31706.0   \n",
              "1709  6.0       33.0  319.0      11.0        319.0       1656.0  31941.0   \n",
              "1710  8.0       29.0  146.0      19.0        224.0       1787.0  16068.0   \n",
              "1711  8.0       16.0  310.0      28.0        278.0       2482.0  33257.0   \n",
              "1712  3.0       32.0  179.0      17.0        108.0       2117.0  26315.0   \n",
              "\n",
              "       adv_id  creat_type_cd  adv_prim_id  ...          pt_d  \\\n",
              "0     21695.0            8.0       1036.0  ...  2.022060e+11   \n",
              "1     12785.0            8.0       1537.0  ...  2.022061e+11   \n",
              "2     13608.0            8.0       1036.0  ...  2.022061e+11   \n",
              "3     22799.0            8.0       1189.0  ...  2.022061e+11   \n",
              "4     21695.0            8.0       1036.0  ...  2.022060e+11   \n",
              "...       ...            ...          ...  ...           ...   \n",
              "1708  12646.0            8.0       1922.0  ...  2.022061e+11   \n",
              "1709  21695.0            8.0       1036.0  ...  2.022060e+11   \n",
              "1710  13876.0            3.0       1234.0  ...  2.022061e+11   \n",
              "1711  15452.0            8.0       1779.0  ...  2.022061e+11   \n",
              "1712  18051.0            8.0       1482.0  ...  2.022061e+11   \n",
              "\n",
              "      u_refreshTimes_x  u_feedLifeCycle_y  u_refreshTimes_y  i_cat  i_upTimes  \\\n",
              "0                  9.0               17.0               9.0   98.0        0.0   \n",
              "1                  9.0               17.0               9.0  140.0        0.0   \n",
              "2                  9.0               17.0               9.0  112.0        0.0   \n",
              "3                  5.0               17.0               5.0  168.0        0.0   \n",
              "4                  6.0               17.0               6.0   16.0        0.0   \n",
              "...                ...                ...               ...    ...        ...   \n",
              "1708               2.0               11.0               2.0  112.0        9.0   \n",
              "1709               4.0               17.0               4.0   98.0        0.0   \n",
              "1710               9.0               17.0               9.0   98.0        0.0   \n",
              "1711               9.0               17.0               9.0   98.0        0.0   \n",
              "1712               0.0               15.0               0.0   98.0        9.0   \n",
              "\n",
              "         e_m    e_pl   user_id  label  \n",
              "0     1277.0  1156.0  269004.0    0.0  \n",
              "1     1319.0  1197.0  104736.0    0.0  \n",
              "2      591.0  2952.0  203902.0    1.0  \n",
              "3      369.0  1701.0  111729.0    1.0  \n",
              "4     1481.0   687.0  102986.0    0.0  \n",
              "...      ...     ...       ...    ...  \n",
              "1708   848.0  1272.0  105735.0    1.0  \n",
              "1709  1277.0  1255.0  134502.0    0.0  \n",
              "1710  1075.0  2073.0  219349.0    0.0  \n",
              "1711   506.0  1422.0  275034.0    0.0  \n",
              "1712    67.0  2620.0  137423.0    1.0  \n",
              "\n",
              "[1713 rows x 24 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_synthetic_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
