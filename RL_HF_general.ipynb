{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h6wxlrA6p9g",
        "outputId": "ed6d8651-3b80-445d-ab1e-bc9beb6492c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: be-great in /opt/conda/envs/llm/lib/python3.10/site-packages (0.0.7)\n",
            "Requirement already satisfied: datasets in /opt/conda/envs/llm/lib/python3.10/site-packages (2.14.6)\n",
            "Requirement already satisfied: transformers in /opt/conda/envs/llm/lib/python3.10/site-packages (4.34.1)\n",
            "Requirement already satisfied: trl in /opt/conda/envs/llm/lib/python3.10/site-packages (0.8.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sdmetric (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sdmetric\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install be-great datasets transformers trl sdmetric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWrM5yCYS21"
      },
      "source": [
        "## Step 0: load dataset\n",
        "\n",
        "First we load the table we want to synthesize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teBDxiXd9Pqv",
        "outputId": "6895e37a-a401-4a1e-c625-bce703a6c26b"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N72uHAKe9lKm"
      },
      "outputs": [],
      "source": [
        "#data_path = \"census1000.csv\"\n",
        "#data_path = \"ChurnModeling.csv\"\n",
        "data_path = \"ctrBalanced500.csv\"\n",
        "data_name = data_path.replace(\".csv\", \"\")\n",
        "test_idx = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GlgwmbSL-JQY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "num_sample = min(500, len(data))\n",
        "if num_sample is not None:\n",
        "    data = data.sample(num_sample, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "1    262\n",
              "0    238\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_data = data\n",
        "target = data.columns[-1]\n",
        "\n",
        "# Assume target is label encoded as 0/1\n",
        "\n",
        "target_encoder = LabelEncoder()\n",
        "balanced_data[target] = target_encoder.fit_transform(balanced_data[target])\n",
        "\n",
        "\n",
        "# Separate the rows where label is 1\n",
        "data_1 = data[data[target] == 1]\n",
        "data_0 = data[data[target] == 0]\n",
        "\n",
        "# Randomly sample from rows where label is 0\n",
        "data_0_sampled = data_0.sample(n=min(len(data_1), len(data_0)), random_state=42)\n",
        "\n",
        "# Combine the two dataframes\n",
        "balanced_data = pd.concat([data_1, data_0_sampled])\n",
        "\n",
        "balanced_data[target].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIIaL3X386HB",
        "outputId": "b0ef44e2-d3cb-4bee-ce45-4a30f9efbe0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500, 24) Index(['age', 'residence', 'city', 'emui_dev', 'device_name', 'device_size',\n",
            "       'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id', 'slot_id',\n",
            "       'spread_app_id', 'hispace_app_tags', 'app_second_class', 'pt_d',\n",
            "       'u_refreshTimes_x', 'u_feedLifeCycle_y', 'u_refreshTimes_y', 'i_cat',\n",
            "       'i_upTimes', 'e_m', 'e_pl', 'user_id', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(balanced_data.shape, balanced_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = train_test_split(balanced_data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSlw-P7NYHIy"
      },
      "source": [
        "## Step 1: supervised-finetuning for table generation\n",
        "\n",
        "In this step, we finetune a distillgpt2 model to perform synthetic table generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "w5zPcP2vYGog",
        "outputId": "ce060eac-b32e-4b70-9c78-d5a1cc7a1469"
      },
      "outputs": [],
      "source": [
        "from be_great import GReaT\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "duration = 500\n",
        "max_seq_len = 500\n",
        "\n",
        "trained_checkpoint = None\n",
        "#trained_checkpoint = \"./great_checkpoint_ctrBalanced500_100\"\n",
        "\n",
        "model_great = GReaT(llm='gpt2', batch_size=32,  epochs=duration, fp16=True,save_steps=30000)\n",
        "if trained_checkpoint is not None:\n",
        "    model_great.load_from_dir(trained_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6500' max='6500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6500/6500 44:20, Epoch 500/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.837200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.651400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.579200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.548600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.521200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.498100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.478000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.462200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.449500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.440800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.434600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.431400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if trained_checkpoint is None:\n",
        "    model_great.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./great_checkpoint_ctrBalanced500_100'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f\"./great_checkpoint_{data_name}_{test_idx}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/be_great/great.py:430: UserWarning: Directory ./great_checkpoint_ctrBalanced500_100 already exists and is overwritten now.\n",
            "  warnings.warn(f\"Directory {path} already exists and is overwritten now.\")\n"
          ]
        }
      ],
      "source": [
        "model_great.save(f\"./great_checkpoint_{data_name}_{test_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = model_great.model\n",
        "base_model.save_pretrained(f\"./trained_base_model_{data_name}_{test_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "465it [00:20, 22.28it/s]                         \n"
          ]
        }
      ],
      "source": [
        "synthetic_data = model_great.sample(n_samples=len(train),max_length=max_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "synthetic_data.to_csv(f\"{data_name}_GReaT_default_{test_idx}.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vrgAcTLYyU9"
      },
      "source": [
        "## Step 2: reward model training\n",
        "\n",
        "Our reward model is a powerful classifier trained on the real tabular data. We apply it on synthetic table rows, and the reward is maximize when the distance between synthetic and predicted class probabilities are minimized. The idea is that the synthetic data should preserved the feature-target relationship as found in the real data, by powerful classsifiers such as XGboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ic4mtv_JY1Rd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.61\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.56      0.61        54\n",
            "           1       0.56      0.67      0.61        46\n",
            "\n",
            "    accuracy                           0.61       100\n",
            "   macro avg       0.62      0.61      0.61       100\n",
            "weighted avg       0.62      0.61      0.61       100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "real_columns = train.columns\n",
        "\n",
        "X_train, X_test = train.iloc[:, :-1], test.iloc[:, :-1]\n",
        "y_train, y_test = train.iloc[:, -1], test.iloc[:, -1]\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the model and preprocessors\n",
        "classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline_real = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier)])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline_real.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline_real.predict(X_test)\n",
        "\n",
        "# Evaluation as a sanity check\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic Accuracy: 0.52\n",
            "Synthetic Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.48      0.52        54\n",
            "           1       0.48      0.57      0.52        46\n",
            "\n",
            "    accuracy                           0.52       100\n",
            "   macro avg       0.52      0.52      0.52       100\n",
            "weighted avg       0.53      0.52      0.52       100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now try fitting a model from synthetic data and evaluate on test set\n",
        "x_synth, y_synth = synthetic_data.iloc[:, :-1],synthetic_data.iloc[:, -1]\n",
        "\n",
        "classifier_synth = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline_synth = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier_synth)])\n",
        "pipeline_synth.fit(x_synth, y_synth)\n",
        "y_pred = pipeline_synth.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Synthetic Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Synthetic Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzfTRB1WZ6IH"
      },
      "source": [
        "## Step 3: reward finetuning\n",
        "\n",
        "Now we define a reward function that does the following\n",
        "\n",
        "\n",
        "1.   Given a generated text, reverse it back to a table row.\n",
        "2.   Use the pre-trained classifier to predict its target based generated features.\n",
        "3.   Given the distance between predicted and synthetic targets, calculate its reward.\n",
        "\n",
        "Then we finetune the model trained above in PPO setting.\n",
        "\n",
        "Cross check to see if PPO and RL work correctly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eQdbycVto_DN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from be_great.great_utils import _convert_text_to_tabular_data, _convert_tokens_to_text\n",
        "\n",
        "\n",
        "def calcualte_reward(synth_data, pipeline):\n",
        "    X_synth = synth_data.iloc[:, :-1]\n",
        "    y_synth = pd.to_numeric(synth_data.iloc[:, -1])\n",
        "    #y_synth = REAL_LABEL_ENCODER.transform(y_synth)\n",
        "\n",
        "    # Apply preprocessing pipeline\n",
        "    #print(X_synth.dtypes)\n",
        "\n",
        "    # Get predicted class probabilities\n",
        "    y_pred_proba = pipeline.predict_proba(X_synth)[:, 1]\n",
        "\n",
        "    rewards = 1 - np.abs(y_pred_proba - y_synth)\n",
        "\n",
        "    # Format rewards for PPO trainer\n",
        "    # the PPO trainer expects a list of tensors\n",
        "    return [torch.tensor(r) for r in rewards]\n",
        "\n",
        "# Custom reward function\n",
        "def reward_function(output_text, pipeline, columns, num_cols):\n",
        "    # Replace with your actual reward computation logic\n",
        "    synth_data = _convert_text_to_tabular_data(output_text, columns)\n",
        "    # Remove rows where we have not generated anything\n",
        "    #synth_data = synth_data[~(synth_data == \"placeholder\").any(axis=1)]\n",
        "\n",
        "    # Remove rows where all values are NaN\n",
        "    #synth_data = synth_data.dropna(how=\"all\")\n",
        "\n",
        "    # Remove rows with flawed numerical values but keep NaNs\n",
        "    #print(len(num_cols), synth_data.shape,num_cols)\n",
        "    for i_num_cols in num_cols:\n",
        "        synth_data[i_num_cols] = pd.to_numeric(\n",
        "            synth_data[i_num_cols], errors=\"coerce\"\n",
        "        )\n",
        "        #print(\"coerced_series\",coerced_series)\n",
        "        #print(\"synth_data[i_num_cols]\",synth_data[i_num_cols])\n",
        "        #print(\"i_num_cols\",i_num_cols)\n",
        "        #synth_data = synth_data[\n",
        "        #    coerced_series.notnull() | synth_data[i_num_cols].isna()\n",
        "        #]\n",
        "    #print(synth_data.shape)\n",
        "\n",
        "    # Convert numerical columns to float\n",
        "    synth_data[num_cols] = synth_data[num_cols].astype(float)\n",
        "    #print(synth_data.dtypes)\n",
        "    if len(synth_data) > 0:\n",
        "        return calcualte_reward(synth_data, pipeline)\n",
        "    else:\n",
        "        return [torch.tensor(0.0, dtype=torch.float32)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oTpKJsBgtRSa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def df_to_string_list(df, fraction_to_select=1):\n",
        "    string_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        # Calculate the number of cells to select based on the fraction\n",
        "        if fraction_to_select < 1:\n",
        "            num_to_select = max(1, int(len(row) * fraction_to_select))  # Ensure at least one cell is selected\n",
        "            \n",
        "            # Randomly select the specified fraction of cells from the row\n",
        "            selected_columns = random.sample(list(row.index), num_to_select)\n",
        "        else:\n",
        "            selected_columns = df.columns\n",
        "        \n",
        "        # Create the row string using only the selected columns\n",
        "        row_string = \", \".join([f\"{col} is {row[col]}\" for col in selected_columns])\n",
        "        string_list.append(row_string)\n",
        "    return string_list\n",
        "\n",
        "def df_cells_to_string_list(df):\n",
        "    cell_string_list = []\n",
        "    for col in df.columns:\n",
        "        for val in df[col]:\n",
        "            cell_string_list.append(f\"{col} is {val}\")\n",
        "    return cell_string_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['age', 'residence', 'city', 'emui_dev', 'device_name', 'device_size',\n",
              "       'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id', 'slot_id',\n",
              "       'spread_app_id', 'hispace_app_tags', 'app_second_class', 'pt_d',\n",
              "       'u_refreshTimes_x', 'u_feedLifeCycle_y', 'u_refreshTimes_y', 'i_cat',\n",
              "       'i_upTimes', 'e_m', 'e_pl', 'user_id'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numerical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'label'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "real_columns[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XEeOHfCl6JbO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7642)\n"
          ]
        }
      ],
      "source": [
        "df_text = df_to_string_list(train)\n",
        "\n",
        "# Test reconstruction of table from text\n",
        "rewards = reward_function(df_text, pipeline_real, real_columns, list(numerical_features))\n",
        "print(torch.mean(torch.tensor([r.item() for r in rewards])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make Training Data In DPO Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "m2aBWGqiZ9Bl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "llm = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the trained base model into AutoModelForCausalLMWithValueHead\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(f\"./trained_base_model_{data_name}_{test_idx}\")\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Define dataset. Use different columns as condition to improve diversity\n",
        "# or use target to simplify training\n",
        "# or use feature to get target\n",
        "#starting_df = train.drop(columns=[target])\n",
        "starting_df = train[[target]]\n",
        "dataset = CustomDataset(df_to_string_list(starting_df,1))\n",
        "batch_size = 8\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define PPO Configuration\n",
        "ppo_config = PPOConfig(\n",
        "    model_name=llm,\n",
        "    learning_rate=5e-7,  # Slightly increased learning rate\n",
        "    batch_size=batch_size,\n",
        "    ppo_epochs=6,\n",
        "    mini_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "# Initialize the PPO Trainer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=model,\n",
        "    ref_model=None,  # Reference model\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xSrnaev_g1XM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -6.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -9.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -24.03 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -19.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -14.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -14.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -5.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -8.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -29.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -44.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -23.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -18.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -47.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -39.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -36.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -30.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -61.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -58.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -20.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -31.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -25.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -5.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -33.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/6, Average Reward: 0.553227961063385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -24.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -34.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -53.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -17.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -75.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -31.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -45.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -22.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -17.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -9.26 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -51.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -51.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -32.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -47.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -225.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -39.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -28.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -42.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -39.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -32.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -77.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -23.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -39.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -5.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -6.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -9.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -76.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -32.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -52.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2/6, Average Reward: 0.5562756061553955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -89.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -71.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -45.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -104.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -84.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -48.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -42.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -54.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -163.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -17.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -45.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -59.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -115.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -31.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -33.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.26 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -166.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -65.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -32.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -68.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -67.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -76.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -31.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -35.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -448.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -165.41 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3/6, Average Reward: 0.5386244058609009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -123.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -60.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -438.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -52.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -41.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -463.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -225.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -210.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -83.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -55.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -154.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"1 0\" at position 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -46.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -8.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -6.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -21.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -387.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -55.69 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -21.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -19.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -16.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -17.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -31.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -31.58 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -14.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -199.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -137.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -52.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -48.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4/6, Average Reward: 0.5276709198951721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -58.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -43.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -44.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -21.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -132.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -192.25 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"0)\" at position 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -18.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -60.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -82.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -56.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"0) pt_d\" at position 5\n",
            "Error: Unable to parse string \"1).0\" at position 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -23.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -64.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"1 1\" at position 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -102.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -111.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -45.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -55.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -204.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -51.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -55.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -102.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -98.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -35.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -41.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -133.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -222.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -143.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -131.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -64.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"0) __index_level_0__\" at position 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -29.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5/6, Average Reward: -2.611746311187744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -96.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -58.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -27.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -168.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -133.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -83.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"1 0\" at position 0\n",
            "Error: Unable to parse string \"1).\" at position 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -169.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -152.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"0) hispace_app_tags\" at position 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -147.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -60.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (11.52) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (28.03) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -266.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (16.51) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (41.26) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (81.79) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (180.53) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (315.49) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (1946.49) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -251.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"1)\" at position 2\n",
            "Error: Unable to parse string \"0 0\" at position 3\n",
            "Error: Unable to parse string \"0)\" at position 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -58.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"0)\" at position 0\n",
            "Error: Unable to parse string \"1 0\" at position 7\n",
            "Error: Unable to parse string \"0); device_name\" at position 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -27.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"1 1\" at position 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (16.68) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (45.88) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -163.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -19.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -190.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -20.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (21.28) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (50.21) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -280.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -27.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"1)\" at position 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"0).\" at position 5\n",
            "Error: Unable to parse string \"1 0\" at position 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -9.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -76.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to parse string \"0).0\" at position 5\n",
            "Error: Unable to parse string \"1 1.0\" at position 2\n",
            "Error: Unable to parse string \"1) pt_d\" at position 0\n",
            "Epoch: 6/6, Average Reward: 0.5482426285743713\n",
            "PPO training completed!\n"
          ]
        }
      ],
      "source": [
        "# PPO Training loop\n",
        "\n",
        "#https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "for epoch in range(ppo_config.ppo_epochs):\n",
        "    epoch_rewards = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        try:\n",
        "            # Tokenize inputs and move to device\n",
        "            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "\n",
        "            # Generate outputs from the model\n",
        "            # Set max_length to the desired length of the generated text\n",
        "            max_length = max_seq_len  # Adjust as needed for your application\n",
        "            generated_ids = model.generate(input_ids, max_length=max_length, do_sample=True,temperature=0.70, pad_token_id=50256)\n",
        "\n",
        "            generated_texts = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(generated_ids.size(0))]\n",
        "\n",
        "            #rewards = [reward_function([generated_text], pipeline, real_columns, list(numerical_features)) for generated_text in generated_texts]\n",
        "            rewards = reward_function(generated_texts, pipeline_real, real_columns, list(numerical_features))\n",
        "            #print(len(generated_texts),len(rewards))\n",
        "\n",
        "            #rewards = [item for sublist in rewards for item in sublist]\n",
        "\n",
        "            epoch_rewards.extend(rewards)\n",
        "\n",
        "            queries = [input_ids[i] for i in range(input_ids.size(0))]\n",
        "            responses = [generated_ids[i] for i in range(generated_ids.size(0))]\n",
        "            rewards = [reward.clone().detach() for reward in rewards]\n",
        "\n",
        "            ppo_trainer.step(queries, responses, rewards)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            continue\n",
        "        #break\n",
        "\n",
        "    avg_reward = torch.mean(torch.stack(epoch_rewards)).item()\n",
        "    print(f\"Epoch: {epoch+1}/{ppo_config.ppo_epochs}, Average Reward: {avg_reward}\")\n",
        "\n",
        "print(\"PPO training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_directory = f\"RL_trained_{data_name}_{test_idx}\"\n",
        "model.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu25Eastzwwp"
      },
      "source": [
        "## Step 4 (TODO)\n",
        "\n",
        "Finally we load the trained parameters back to GReaT model, generate synthetic data, train another XGBoost on new synthtic data and observe changes its utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at RL_trained_census1000_100 were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Run this cell if we are returning after RL training.\n",
        "save_directory = f\"RL_trained_{data_name}_{test_idx}\"\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(save_directory)\n",
        "\n",
        "llm = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za5h7cGl8LaZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "188it [00:03, 59.82it/s]                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6004)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model_great.parameters = model.parameters\n",
        "new_synthetic_data = model_great.sample(n_samples=len(train),max_length=duration)\n",
        "\n",
        "new_df_text = df_to_string_list(new_synthetic_data)\n",
        "\n",
        "new_rewards = reward_function(new_df_text, pipeline, real_columns, list(numerical_features)+[real_columns[-1]])\n",
        "\n",
        "print(torch.mean(torch.stack(new_rewards)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_synthetic_data.to_csv(f\"{data_name}_GReaTRL_default_{test_idx}.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6363636363636364\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.91      0.71        22\n",
            "           1       0.80      0.36      0.50        22\n",
            "\n",
            "    accuracy                           0.64        44\n",
            "   macro avg       0.69      0.64      0.61        44\n",
            "weighted avg       0.69      0.64      0.61        44\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_synth_new, y_synth_new = new_synthetic_data.iloc[:,:-1], new_synthetic_data.iloc[:,-1]\n",
        "\n",
        "\n",
        "classifier_synth = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline_synth_new = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier_synth)])\n",
        "\n",
        "pipeline_synth_new.fit(x_synth_new, y_synth_new)\n",
        "y_pred = pipeline_synth_new.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "# Evaluation as sanity check\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>residence</th>\n",
              "      <th>city</th>\n",
              "      <th>emui_dev</th>\n",
              "      <th>device_name</th>\n",
              "      <th>device_size</th>\n",
              "      <th>task_id</th>\n",
              "      <th>adv_id</th>\n",
              "      <th>creat_type_cd</th>\n",
              "      <th>adv_prim_id</th>\n",
              "      <th>slot_id</th>\n",
              "      <th>...</th>\n",
              "      <th>pt_d</th>\n",
              "      <th>u_refreshTimes_x</th>\n",
              "      <th>u_feedLifeCycle_y</th>\n",
              "      <th>u_refreshTimes_y</th>\n",
              "      <th>i_cat</th>\n",
              "      <th>i_upTimes</th>\n",
              "      <th>e_m</th>\n",
              "      <th>e_pl</th>\n",
              "      <th>user_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>2032.0</td>\n",
              "      <td>17116.0</td>\n",
              "      <td>21776.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1852.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1509.0</td>\n",
              "      <td>212965.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.0</td>\n",
              "      <td>297.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>31706.0</td>\n",
              "      <td>12646.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>705.0</td>\n",
              "      <td>2835.0</td>\n",
              "      <td>189469.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>2032.0</td>\n",
              "      <td>21812.0</td>\n",
              "      <td>18340.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2066.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1444.0</td>\n",
              "      <td>1853.0</td>\n",
              "      <td>124834.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21.0</td>\n",
              "      <td>291.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>351.0</td>\n",
              "      <td>2032.0</td>\n",
              "      <td>26452.0</td>\n",
              "      <td>22532.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1524.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>2182.0</td>\n",
              "      <td>148931.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.0</td>\n",
              "      <td>328.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>1656.0</td>\n",
              "      <td>28290.0</td>\n",
              "      <td>13224.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>7.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1482.0</td>\n",
              "      <td>332.0</td>\n",
              "      <td>241069.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1708</th>\n",
              "      <td>39.0</td>\n",
              "      <td>429.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>12092.0</td>\n",
              "      <td>16444.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1482.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1194.0</td>\n",
              "      <td>2999.0</td>\n",
              "      <td>203992.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1709</th>\n",
              "      <td>21.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>19358.0</td>\n",
              "      <td>19175.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1909.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1277.0</td>\n",
              "      <td>656.0</td>\n",
              "      <td>216007.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1710</th>\n",
              "      <td>21.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>222881.0</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1557.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1097.0</td>\n",
              "      <td>797.0</td>\n",
              "      <td>260086.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1711</th>\n",
              "      <td>33.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>2032.0</td>\n",
              "      <td>33257.0</td>\n",
              "      <td>13886.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>591.0</td>\n",
              "      <td>2893.0</td>\n",
              "      <td>206589.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1712</th>\n",
              "      <td>33.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>2469.0</td>\n",
              "      <td>33083.0</td>\n",
              "      <td>18274.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1187.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>494.0</td>\n",
              "      <td>109716.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1713 rows  23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      residence   city  emui_dev  device_name  device_size   task_id   adv_id  \\\n",
              "0          26.0  172.0      20.0        248.0       2032.0   17116.0  21776.0   \n",
              "1          18.0  297.0      21.0        183.0       2401.0   31706.0  12646.0   \n",
              "2          29.0  116.0      20.0        153.0       2032.0   21812.0  18340.0   \n",
              "3          21.0  291.0      35.0        351.0       2032.0   26452.0  22532.0   \n",
              "4          20.0  328.0      29.0        337.0       1656.0   28290.0  13224.0   \n",
              "...         ...    ...       ...          ...          ...       ...      ...   \n",
              "1708       39.0  429.0      21.0        333.0       2117.0   12092.0  16444.0   \n",
              "1709       21.0  220.0      35.0        319.0       2117.0   19358.0  19175.0   \n",
              "1710       21.0  434.0      37.0        252.0       2117.0  222881.0  12620.0   \n",
              "1711       33.0  319.0      11.0        151.0       2032.0   33257.0  13886.0   \n",
              "1712       33.0  319.0      11.0        248.0       2469.0   33083.0  18274.0   \n",
              "\n",
              "      creat_type_cd  adv_prim_id  slot_id  ...          pt_d  \\\n",
              "0               8.0       1852.0     17.0  ...  2.022061e+11   \n",
              "1               8.0       1036.0     54.0  ...  2.022060e+11   \n",
              "2               8.0       2066.0     38.0  ...  2.022061e+11   \n",
              "3               2.0       1524.0     38.0  ...  2.022061e+11   \n",
              "4               8.0       1036.0     17.0  ...  2.022061e+11   \n",
              "...             ...          ...      ...  ...           ...   \n",
              "1708            3.0       1482.0     54.0  ...  2.022061e+11   \n",
              "1709            8.0       1909.0     59.0  ...  2.022060e+11   \n",
              "1710            3.0       1557.0     26.0  ...  2.022061e+11   \n",
              "1711           10.0       1036.0     16.0  ...  2.022061e+11   \n",
              "1712           10.0       1187.0     16.0  ...  2.022060e+11   \n",
              "\n",
              "      u_refreshTimes_x  u_feedLifeCycle_y  u_refreshTimes_y  i_cat  i_upTimes  \\\n",
              "0                  0.0               16.0               0.0   98.0        0.0   \n",
              "1                  5.0               17.0               5.0   98.0        9.0   \n",
              "2                  8.0               17.0               8.0   98.0        9.0   \n",
              "3                  6.0               17.0               6.0  171.0        0.0   \n",
              "4                  7.0               17.0               7.0   98.0        9.0   \n",
              "...                ...                ...               ...    ...        ...   \n",
              "1708               0.0               17.0               0.0   17.0        9.0   \n",
              "1709               8.0               17.0               8.0   98.0        9.0   \n",
              "1710               8.0               17.0               7.0   98.0        0.0   \n",
              "1711               6.0               17.0               6.0   98.0        0.0   \n",
              "1712               8.0               17.0               8.0  168.0        0.0   \n",
              "\n",
              "         e_m    e_pl   user_id  label  \n",
              "0       73.0  1509.0  212965.0    0.0  \n",
              "1      705.0  2835.0  189469.0    0.0  \n",
              "2     1444.0  1853.0  124834.0    1.0  \n",
              "3     1205.0  2182.0  148931.0    1.0  \n",
              "4     1482.0   332.0  241069.0    1.0  \n",
              "...      ...     ...       ...    ...  \n",
              "1708  1194.0  2999.0  203992.0    0.0  \n",
              "1709  1277.0   656.0  216007.0    0.0  \n",
              "1710  1097.0   797.0  260086.0    0.0  \n",
              "1711   591.0  2893.0  206589.0    0.0  \n",
              "1712    73.0   494.0  109716.0    1.0  \n",
              "\n",
              "[1713 rows x 23 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_synthetic_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
