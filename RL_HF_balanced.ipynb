{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h6wxlrA6p9g",
        "outputId": "ed6d8651-3b80-445d-ab1e-bc9beb6492c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: be-great in /opt/conda/envs/llm/lib/python3.10/site-packages (0.0.7)\n",
            "Requirement already satisfied: datasets in /opt/conda/envs/llm/lib/python3.10/site-packages (2.14.6)\n",
            "Requirement already satisfied: transformers in /opt/conda/envs/llm/lib/python3.10/site-packages (4.34.1)\n",
            "Requirement already satisfied: trl in /opt/conda/envs/llm/lib/python3.10/site-packages (0.8.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sdmetric (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sdmetric\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install be-great datasets transformers trl sdmetric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWrM5yCYS21"
      },
      "source": [
        "## Step 0: load dataset\n",
        "\n",
        "First we load the table we want to synthesize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teBDxiXd9Pqv",
        "outputId": "6895e37a-a401-4a1e-c625-bce703a6c26b"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N72uHAKe9lKm"
      },
      "outputs": [],
      "source": [
        "#data_path = \"/content/gdrive/MyDrive/stats261/ctr_subset.csv\"\n",
        "data_path = \"ctr_subset_imbalanced.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GlgwmbSL-JQY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv(data_path,index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YjUH_pxfYVAV"
      },
      "outputs": [],
      "source": [
        "# Adjust order of columns to make target label the last\n",
        "labels = data.pop('label')\n",
        "data['label'] = labels\n",
        "real_columns = data.columns\n",
        "\n",
        "# Separate the rows where label is 1\n",
        "data_1 = data[data['label'] == 1]\n",
        "data_0 = data[data['label'] == 0]\n",
        "\n",
        "# Randomly sample from rows where label is 0\n",
        "data_0_sampled = data_0.sample(n=len(data_1), random_state=42)\n",
        "\n",
        "# Combine the two dataframes\n",
        "balanced_data = pd.concat([data_1, data_0_sampled])\n",
        "\n",
        "train, test = train_test_split(balanced_data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "1    2773\n",
              "0    2773\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "balanced_data.to_csv(\"ctrTaskBalanced.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIIaL3X386HB",
        "outputId": "b0ef44e2-d3cb-4bee-ce45-4a30f9efbe0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5546, 24) Index(['age', 'residence', 'city', 'series_dev', 'series_group', 'emui_dev',\n",
            "       'device_name', 'device_size', 'slot_id', 'pt_d', 'u_refreshTimes_x',\n",
            "       'u_feedLifeCycle_x', 'u_browserLifeCycle', 'u_browserMode',\n",
            "       'u_feedLifeCycle_y', 'u_refreshTimes_y', 'i_cat', 'i_dislikeTimes',\n",
            "       'i_upTimes', 'e_m', 'e_po', 'e_pl', 'user_id', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(balanced_data.shape, balanced_data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSlw-P7NYHIy"
      },
      "source": [
        "## Step 1: supervised-finetuning for table generation\n",
        "\n",
        "In this step, we finetune a distillgpt2 model to perform synthetic table generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "w5zPcP2vYGog",
        "outputId": "ce060eac-b32e-4b70-9c78-d5a1cc7a1469"
      },
      "outputs": [],
      "source": [
        "from be_great import GReaT\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "duration = 500\n",
        "max_seq_len = 500\n",
        "\n",
        "#trained_checkpoint = None\n",
        "trained_checkpoint = \"./great_checkpoint_balanced\"\n",
        "\n",
        "model_great = GReaT(llm='distilgpt2', batch_size=32,  epochs=duration, fp16=True,save_steps=30000)\n",
        "if trained_checkpoint is not None:\n",
        "    model_great.load_from_dir(trained_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='69500' max='69500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [69500/69500 5:14:41, Epoch 500/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.642000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.626900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.615000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.577100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.567700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.560900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.554700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.550400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.545900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.541800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.538400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.535000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.532100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.529200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.526600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.524000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.521500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.517200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.514900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.513000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.510700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.508700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.504300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.502700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.500400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.498700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.494600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.493300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.491100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.489400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.487600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.485800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.484200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.482100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.480700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.478900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.477400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.474200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.472300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.470700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.467600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.466400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.464800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.463300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.462300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.460800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.459000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.457700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.455100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.453800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.448600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.446400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.443900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.442800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.442000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.440200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.437900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.436800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.436100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.435200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.434400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.433300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.432500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>0.431600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>0.429700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>0.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>0.428400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>0.427800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>0.427200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>0.425800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>0.425300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>0.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>0.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>0.423100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>0.422400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>0.421800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>0.421300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>0.420600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>0.420300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>0.419600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>0.418700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>0.418200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>0.417900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>0.417200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>0.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>0.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>0.416000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>0.415000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>0.415000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>0.414100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>0.413500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>0.413100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>0.413400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>0.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56500</td>\n",
              "      <td>0.412400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>0.411700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57500</td>\n",
              "      <td>0.411900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>0.410900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58500</td>\n",
              "      <td>0.411000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>0.410600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59500</td>\n",
              "      <td>0.410800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>0.409700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60500</td>\n",
              "      <td>0.409700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61000</td>\n",
              "      <td>0.409700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61500</td>\n",
              "      <td>0.409400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62000</td>\n",
              "      <td>0.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62500</td>\n",
              "      <td>0.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63000</td>\n",
              "      <td>0.408500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63500</td>\n",
              "      <td>0.408100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64000</td>\n",
              "      <td>0.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64500</td>\n",
              "      <td>0.408200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65000</td>\n",
              "      <td>0.407700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65500</td>\n",
              "      <td>0.407800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66000</td>\n",
              "      <td>0.407800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66500</td>\n",
              "      <td>0.407500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67000</td>\n",
              "      <td>0.407300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67500</td>\n",
              "      <td>0.407100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68000</td>\n",
              "      <td>0.406900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68500</td>\n",
              "      <td>0.407000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69000</td>\n",
              "      <td>0.406800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69500</td>\n",
              "      <td>0.406900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<be_great.great_trainer.GReaTTrainer at 0x7f79713375e0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_great.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llm/lib/python3.10/site-packages/be_great/great.py:430: UserWarning: Directory ./great_checkpoint_balanced already exists and is overwritten now.\n",
            "  warnings.warn(f\"Directory {path} already exists and is overwritten now.\")\n"
          ]
        }
      ],
      "source": [
        "model_great.save(\"./great_checkpoint_balanced\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = model_great.model\n",
        "base_model.save_pretrained(\"./trained_base_model_balanced\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4528it [01:45, 43.02it/s]                          \n"
          ]
        }
      ],
      "source": [
        "synthetic_data = model_great.sample(n_samples=len(train),max_length=max_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "synthetic_data.to_csv(\"ctrTaskBalanced_GReaT_default_5546.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vrgAcTLYyU9"
      },
      "source": [
        "## Step 2: reward model training\n",
        "\n",
        "Our reward model is a powerful classifier trained on the real tabular data. We apply it on synthetic table rows, and the reward is maximize when the distance between synthetic and predicted class probabilities are minimized. The idea is that the synthetic data should preserved the feature-target relationship as found in the real data, by powerful classsifiers such as XGboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ic4mtv_JY1Rd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8162162162162162\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.82       542\n",
            "           1       0.83      0.80      0.82       568\n",
            "\n",
            "    accuracy                           0.82      1110\n",
            "   macro avg       0.82      0.82      0.82      1110\n",
            "weighted avg       0.82      0.82      0.82      1110\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "\n",
        "X_train, X_test = train.iloc[:, :-1], test.iloc[:, :-1]\n",
        "y_train, y_test = train.iloc[:, -1],  test.iloc[:, -1]  # assume y is already encoded as 0 and 1\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "\n",
        "# Define the model and preprocessors\n",
        "classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier)])\n",
        "\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation as sanity check\n",
        "y_pred = pipeline.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic Accuracy: 0.7684684684684685\n",
            "Synthetic Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.69      0.74       542\n",
            "           1       0.74      0.85      0.79       568\n",
            "\n",
            "    accuracy                           0.77      1110\n",
            "   macro avg       0.77      0.77      0.77      1110\n",
            "weighted avg       0.77      0.77      0.77      1110\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now try fitting a model from synthetic data and evaluate on test set\n",
        "x_synth, y_synth = synthetic_data.iloc[:, :-1],synthetic_data.iloc[:, -1]\n",
        "\n",
        "classifier_synth = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline_synth = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier_synth)])\n",
        "pipeline_synth.fit(x_synth, y_synth)\n",
        "y_pred = pipeline_synth.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Synthetic Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Synthetic Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzfTRB1WZ6IH"
      },
      "source": [
        "## Step 3: reward finetuning\n",
        "\n",
        "Now we define a reward function that does the following\n",
        "\n",
        "\n",
        "1.   Given a generated text, reverse it back to a table row.\n",
        "2.   Use the pre-trained classifier to predict its target based generated features.\n",
        "3.   Given the distance between predicted and synthetic targets, calculate its reward.\n",
        "\n",
        "Then we finetune the model trained above in PPO setting.\n",
        "\n",
        "Cross check to see if PPO and RL work correctly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eQdbycVto_DN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from be_great.great_utils import _convert_text_to_tabular_data, _convert_tokens_to_text\n",
        "\n",
        "\n",
        "def calcualte_reward(synth_data, classifier, preprocessor):\n",
        "    X_synth = synth_data.iloc[:, :-1]\n",
        "    y_synth = synth_data.iloc[:, -1]\n",
        "\n",
        "    # Apply preprocessing pipeline\n",
        "    #print(X_synth)\n",
        "    X_synth_transformed = preprocessor.transform(X_synth)\n",
        "\n",
        "    # Get predicted class probabilities\n",
        "    y_pred_proba = classifier.predict_proba(X_synth_transformed)[:, 1]\n",
        "\n",
        "    rewards = 1 - np.abs(y_pred_proba - y_synth)\n",
        "\n",
        "    # Format rewards for PPO trainer\n",
        "    # the PPO trainer expects a list of tensors\n",
        "    return [torch.tensor(r) for r in rewards]\n",
        "\n",
        "# Custom reward function\n",
        "def reward_function(output_text, clasifier, preprocessor, columns, num_cols):\n",
        "    # Replace with your actual reward computation logic\n",
        "    synth_data = _convert_text_to_tabular_data(output_text, columns)\n",
        "    # Remove rows where we have not generated anything\n",
        "    synth_data = synth_data[~(synth_data == \"placeholder\").any(axis=1)]\n",
        "\n",
        "    # Remove rows where all values are NaN\n",
        "    synth_data = synth_data.dropna(how=\"all\")\n",
        "\n",
        "    # Remove rows with flawed numerical values but keep NaNs\n",
        "    #print(len(num_cols), synth_data.shape)\n",
        "    for i_num_cols in num_cols:\n",
        "        coerced_series = pd.to_numeric(\n",
        "            synth_data[i_num_cols], errors=\"coerce\"\n",
        "        )\n",
        "        synth_data = synth_data[\n",
        "            coerced_series.notnull() | synth_data[i_num_cols].isna()\n",
        "        ]\n",
        "\n",
        "    # Convert numerical columns to float\n",
        "    synth_data[num_cols] = synth_data[num_cols].astype(float)\n",
        "    if len(synth_data) > 0:\n",
        "        return calcualte_reward(synth_data, clasifier, preprocessor)\n",
        "    else:\n",
        "        return [torch.tensor(0.0, dtype=torch.float32)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oTpKJsBgtRSa"
      },
      "outputs": [],
      "source": [
        "def df_to_string_list(df):\n",
        "    string_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        row_string = \", \".join([f\"{col} is {val}\" for col, val in row.items()])\n",
        "        string_list.append(row_string)\n",
        "    return string_list\n",
        "\n",
        "def df_cells_to_string_list(df):\n",
        "    cell_string_list = []\n",
        "    for col in df.columns:\n",
        "        for val in df[col]:\n",
        "            cell_string_list.append(f\"{col} is {val}\")\n",
        "    return cell_string_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['age', 'residence', 'city', 'series_dev', 'series_group', 'emui_dev',\n",
              "       'device_name', 'device_size', 'slot_id', 'pt_d', 'u_refreshTimes_x',\n",
              "       'u_feedLifeCycle_x', 'u_browserLifeCycle', 'u_browserMode',\n",
              "       'u_feedLifeCycle_y', 'u_refreshTimes_y', 'i_cat', 'i_dislikeTimes',\n",
              "       'i_upTimes', 'e_m', 'e_po', 'e_pl', 'user_id'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numerical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'label'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "real_columns[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XEeOHfCl6JbO"
      },
      "outputs": [],
      "source": [
        "df_text = df_to_string_list(synthetic_data)\n",
        "\n",
        "# Test reconstruction of table from text\n",
        "rewards = reward_function(df_text, classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "m2aBWGqiZ9Bl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "llm = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the trained base model into AutoModelForCausalLMWithValueHead\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"./trained_base_model_balanced\")\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Define dataset. Use different columns as condition to improve diversity\n",
        "dataset = CustomDataset(df_cells_to_string_list(train))\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define PPO Configuration\n",
        "ppo_config = PPOConfig(\n",
        "    model_name=llm,\n",
        "    learning_rate=1e-5,  # Slightly increased learning rate\n",
        "    batch_size=batch_size,\n",
        "    ppo_epochs=4,\n",
        "    mini_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "# Initialize the PPO Trainer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=model,\n",
        "    ref_model=None,  # Reference model\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "xSrnaev_g1XM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/4, Average Reward: 0.0025789199862629175\n",
            "Epoch: 2/4, Average Reward: 0.0\n",
            "Epoch: 3/4, Average Reward: 0.0\n",
            "Epoch: 4/4, Average Reward: 0.0\n",
            "PPO training completed!\n"
          ]
        }
      ],
      "source": [
        "# PPO Training loop\n",
        "\n",
        "#https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "for epoch in range(ppo_config.ppo_epochs):\n",
        "    epoch_rewards = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        try:\n",
        "            # Tokenize inputs and move to device\n",
        "            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "\n",
        "            # Generate outputs from the model\n",
        "            # Set max_length to the desired length of the generated text\n",
        "            max_length = 300  # Adjust as needed for your application\n",
        "            generated_ids = model.generate(input_ids, max_length=max_length, do_sample=True,temperature=0.70, pad_token_id=50256)\n",
        "\n",
        "            generated_texts = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(generated_ids.size(0))]\n",
        "\n",
        "            rewards = [reward_function([generated_text], classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]]) for generated_text in generated_texts]\n",
        "\n",
        "            rewards = [item for sublist in rewards for item in sublist]\n",
        "\n",
        "            epoch_rewards.extend(rewards)\n",
        "\n",
        "            queries = [input_ids[i] for i in range(input_ids.size(0))]\n",
        "            responses = [generated_ids[i] for i in range(generated_ids.size(0))]\n",
        "            rewards = [reward.clone().detach() for reward in rewards]\n",
        "\n",
        "            ppo_trainer.step(queries, responses, rewards)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    avg_reward = torch.mean(torch.stack(epoch_rewards)).item()\n",
        "    print(f\"Epoch: {epoch+1}/{ppo_config.ppo_epochs}, Average Reward: {avg_reward}\")\n",
        "\n",
        "print(\"PPO training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_directory = \"RL_trained_balanced\"\n",
        "model.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu25Eastzwwp"
      },
      "source": [
        "## Step 4 (TODO)\n",
        "\n",
        "Finally we load the trained parameters back to GReaT model, generate synthetic data, train another XGBoost on new synthtic data and observe changes its utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at RL_trained_balanced were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Run this cell if we are returning after RL training.\n",
        "save_directory = \"RL_trained_balanced\"\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(save_directory)\n",
        "\n",
        "llm = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "za5h7cGl8LaZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4523it [01:44, 43.47it/s]                          \n"
          ]
        }
      ],
      "source": [
        "model_great.parameters = model.parameters\n",
        "new_synthetic_data = model_great.sample(n_samples=len(train),max_length=duration)\n",
        "\n",
        "new_df_text = df_to_string_list(new_synthetic_data)\n",
        "\n",
        "new_rewards = reward_function(df_text, classifier, preprocessor, real_columns, list(numerical_features)+[real_columns[-1]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_synthetic_data.to_csv(\"ctrTaskBalanced_GReaTRL_default_5546.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7810810810810811\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.71      0.76       542\n",
            "           1       0.76      0.85      0.80       568\n",
            "\n",
            "    accuracy                           0.78      1110\n",
            "   macro avg       0.79      0.78      0.78      1110\n",
            "weighted avg       0.78      0.78      0.78      1110\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_synth_new, y_synth_new = new_synthetic_data.iloc[:,:-1], new_synthetic_data.iloc[:,-1]\n",
        "\n",
        "classifier_synth = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "pipeline_synth = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', classifier_synth)])\n",
        "\n",
        "pipeline_synth.fit(x_synth_new, y_synth_new)\n",
        "y_pred = pipeline_synth.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "# Evaluation as sanity check\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>residence</th>\n",
              "      <th>city</th>\n",
              "      <th>series_dev</th>\n",
              "      <th>series_group</th>\n",
              "      <th>emui_dev</th>\n",
              "      <th>device_name</th>\n",
              "      <th>device_size</th>\n",
              "      <th>slot_id</th>\n",
              "      <th>pt_d</th>\n",
              "      <th>...</th>\n",
              "      <th>u_feedLifeCycle_y</th>\n",
              "      <th>u_refreshTimes_y</th>\n",
              "      <th>i_cat</th>\n",
              "      <th>i_dislikeTimes</th>\n",
              "      <th>i_upTimes</th>\n",
              "      <th>e_m</th>\n",
              "      <th>e_po</th>\n",
              "      <th>e_pl</th>\n",
              "      <th>user_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1319.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>607.0</td>\n",
              "      <td>125659.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>372.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>351.0</td>\n",
              "      <td>1656.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>216.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1705.0</td>\n",
              "      <td>269415.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>215.0</td>\n",
              "      <td>2103.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1025.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1347.0</td>\n",
              "      <td>244655.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>429.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>346.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2883.0</td>\n",
              "      <td>240624.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>351.0</td>\n",
              "      <td>1656.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1350.0</td>\n",
              "      <td>213802.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4431</th>\n",
              "      <td>7.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>2451.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>591.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2478.0</td>\n",
              "      <td>123769.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4432</th>\n",
              "      <td>3.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>217.0</td>\n",
              "      <td>1565.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1347.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>244655.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4433</th>\n",
              "      <td>2.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>328.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>2032.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>190725.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4434</th>\n",
              "      <td>2.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>297.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1505.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>2.022061e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1364.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>723.0</td>\n",
              "      <td>270369.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4435</th>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>388.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>1078.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.022060e+11</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1482.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>101988.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4436 rows  24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  residence   city  series_dev  series_group  emui_dev  device_name  \\\n",
              "0     8.0       20.0  170.0        16.0           5.0      35.0        324.0   \n",
              "1     3.0       20.0  372.0        30.0           3.0      35.0        351.0   \n",
              "2     8.0       21.0  220.0        11.0           8.0      20.0        215.0   \n",
              "3     8.0       39.0  429.0        31.0           3.0      20.0        346.0   \n",
              "4     6.0       33.0  319.0        27.0           2.0      11.0        351.0   \n",
              "...   ...        ...    ...         ...           ...       ...          ...   \n",
              "4431  7.0       16.0  120.0        31.0           3.0      21.0        151.0   \n",
              "4432  3.0       26.0  380.0        34.0           7.0      20.0        217.0   \n",
              "4433  2.0       20.0  328.0        17.0           4.0      18.0        288.0   \n",
              "4434  2.0       18.0  297.0        21.0           4.0      30.0        125.0   \n",
              "4435  3.0       11.0  388.0        11.0           8.0      29.0        337.0   \n",
              "\n",
              "      device_size  slot_id          pt_d  ...  u_feedLifeCycle_y  \\\n",
              "0          2401.0     54.0  2.022060e+11  ...               17.0   \n",
              "1          1656.0     16.0  2.022061e+11  ...               17.0   \n",
              "2          2103.0     16.0  2.022061e+11  ...               17.0   \n",
              "3          2117.0     22.0  2.022061e+11  ...               17.0   \n",
              "4          1656.0     16.0  2.022060e+11  ...               17.0   \n",
              "...           ...      ...           ...  ...                ...   \n",
              "4431       2451.0     17.0  2.022060e+11  ...               11.0   \n",
              "4432       1565.0     38.0  2.022060e+11  ...               17.0   \n",
              "4433       2032.0     16.0  2.022060e+11  ...               17.0   \n",
              "4434       1505.0     40.0  2.022061e+11  ...               15.0   \n",
              "4435       1078.0     16.0  2.022060e+11  ...               17.0   \n",
              "\n",
              "      u_refreshTimes_y  i_cat  i_dislikeTimes  i_upTimes     e_m  e_po  \\\n",
              "0                  7.0   98.0             0.0        9.0  1319.0   6.0   \n",
              "1                  7.0  216.0             0.0        0.0  1205.0   6.0   \n",
              "2                  6.0  108.0             0.0        9.0  1025.0   6.0   \n",
              "3                  6.0   98.0             0.0        9.0   565.0   6.0   \n",
              "4                  6.0   98.0             0.0        9.0  1205.0   6.0   \n",
              "...                ...    ...             ...        ...     ...   ...   \n",
              "4431               0.0   10.0             0.0        9.0   591.0   6.0   \n",
              "4432               4.0  219.0             0.0        9.0  1347.0   6.0   \n",
              "4433               8.0  199.0             0.0        0.0  1300.0   2.0   \n",
              "4434               0.0  218.0             5.0        9.0  1364.0   7.0   \n",
              "4435               7.0  171.0             0.0        0.0  1482.0   6.0   \n",
              "\n",
              "        e_pl   user_id  label  \n",
              "0      607.0  125659.0    1.0  \n",
              "1     1705.0  269415.0    0.0  \n",
              "2     1347.0  244655.0    0.0  \n",
              "3     2883.0  240624.0    1.0  \n",
              "4     1350.0  213802.0    0.0  \n",
              "...      ...       ...    ...  \n",
              "4431  2478.0  123769.0    1.0  \n",
              "4432  2016.0  244655.0    1.0  \n",
              "4433   140.0  190725.0    1.0  \n",
              "4434   723.0  270369.0    1.0  \n",
              "4435   103.0  101988.0    0.0  \n",
              "\n",
              "[4436 rows x 24 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_synthetic_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
